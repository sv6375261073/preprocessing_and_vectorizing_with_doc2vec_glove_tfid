{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
    "import pickle\n",
    "import numpy as np\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing numbers , special character , stopwords , single character, 2 char based words  \n",
    "def number_special_stop(text,stop):\n",
    "    text=str(text).lower()\n",
    "    text=re.sub('[\\d\\W]',' ',text)\n",
    "    words=text.split(' ')\n",
    "    text=' '.join([word for word in words if len(word)>2 and word not in stop])\n",
    "    return text\n",
    "\n",
    "#  findingout stemming words, lemmatized words, pos_tag of each word\n",
    "def stem_lemma_posTag(words):\n",
    "    stemmer=nltk.stem.PorterStemmer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # stemming\n",
    "    if type(words) is list:   # checking  argument is list or text line\n",
    "        words=list(set(words))   # unique words in word_list\n",
    "    else:\n",
    "        words=list(set(text.split(' ')))   # making list of words \n",
    "    stem_words=[stemmer.stem(word.strip()) for word in words]\n",
    "    stem_text=' '.join(stem_words)\n",
    "    # lemmatization\n",
    "    try:\n",
    "        lemma_words=[lemmatizer.lemmatize(word.strip()) for word in words]\n",
    "        lemma_text=' '.join(lemma_words)\n",
    "    except LookupError:\n",
    "        nltk.download('wordnet')   # download if not available \n",
    "        lemma_words=[lemmatizer.lemmatize(word.strip()) for word in words]\n",
    "        lemma_text=' '.join(lemma_words)\n",
    "    # pos_tag\n",
    "    try:\n",
    "        pos_tags=nltk.pos_tag(words)\n",
    "    except LookupError:\n",
    "        nltk.download('averaged_perceptron_tagger')\n",
    "        pos_tags=nltk.pos_tag(words)\n",
    "    return stem_text,lemma_text,pos_tags\n",
    "\n",
    "# finding frequency of each unique words \n",
    "def frequency_word(words):\n",
    "    if type(words) is list:\n",
    "        words=words\n",
    "    else:\n",
    "        words=words.split(' ')\n",
    "    freq={word:words.count(word) for word in set(words)}\n",
    "    return freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/home/shiva/Desktop/sample1/'\n",
    "files=os.listdir(path)\n",
    "f=open(os.path.join(path,files[2]),'r',encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_stopwords = ['cir','nos','vii','llc']\n",
    "try:\n",
    "    stop = nltk.corpus.stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    stop = nltk.corpus.stopwords.words('english')\n",
    "for st_word in ex_stopwords:\n",
    "    stop.append(st_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=number_special_stop(f,stop)\n",
    "stem,lemma,pos=stem_lemma_posTag(text)\n",
    "freq=frequency_word(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tfidf:\n",
    "    \"\"\"\n",
    "        initilizing tfid model \n",
    "    \"\"\"\n",
    "    def __init__(self,max_feature=144):\n",
    "        \n",
    "        self.tfid=TfidfVectorizer(ngram_range=(1,2),norm='l1',max_features=max_feature)\n",
    "    def vectorize(self,data):\n",
    "        try:\n",
    "            self.data=self.tfid.fit_transform(data)\n",
    "            self.data=self.data.toarray()\n",
    "        except ValueError:\n",
    "            return \"plese use as: ['texts'] for single sample\"\n",
    "        return self.data\n",
    "    \n",
    "class word_vector:\n",
    "    \"\"\"\n",
    "        model_name: this cunstructor gives you flexibility to use pretrained models\n",
    "        Ex: \n",
    "        1 model_name='glove'\n",
    "        2 model_name='word2vec_trained'\n",
    "        \n",
    "        use own model by training \n",
    "        Note: Default is to train own model\n",
    "        you can use according requirement of your data\n",
    "        \n",
    "        \"\"\"\n",
    "    def __init__(self,model_name=None):  # initilize glove and word2vec model\n",
    "        self.model_name=model_name\n",
    "        if self.model_name=='glove':\n",
    "            self.model=pickle.load(open('glove_model.pkl','rb')) # glove pickle model\n",
    "            print(' glove model loaded .. ')\n",
    "        elif self.model_name=='word2vec_trained':\n",
    "            self.model=gensim.models.KeyedVectors.load_word2vec_format('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "            print(' word2vec_trained model loaded .. ')\n",
    "        else:\n",
    "            self.model=gensim.models.Word2Vec(sentences,window=3,workers=4, min_count=2)\n",
    "            print('you need to train model first on own corpus !!')\n",
    "        \n",
    "    def vectorize(self,words_list): # taking glove model default \n",
    "        \"\"\"\n",
    "          word_list:2D list of words\n",
    "          Ex:[['hi'],['bye','wow']]\n",
    "        \"\"\"\n",
    "        self.vectors=[]    # returning vectorized data \n",
    "        self.count=0       # counting special words \n",
    "        self.special_words=[]  # list for those words which does n't exist in model \n",
    "        for words in words_list:\n",
    "            vec=[]\n",
    "            for word in words:   # required 1D words only for vectorizing  \n",
    "                try:\n",
    "                    vec.append(self.model.get_vector(word).mean()) # 1 value for each word ,by mean of getting model vector\n",
    "                except KeyError:\n",
    "                    print('{} : Not found word : {} '.format(self.count,word))\n",
    "                    self.count+=1\n",
    "                    self.special_words.append(word) # list of special words which didn't found in model\n",
    "            vec=np.array(vec)\n",
    "            vec.resize(144)\n",
    "            self.vectors.append(vec)\n",
    "        return np.array(self.vectors) \n",
    "    def train(self,corpus,epochs=10):\n",
    "        \"\"\"\n",
    "          corpus:2D list of words\n",
    "          Ex:[['hi'],['bye','wow']]\n",
    "        \"\"\"\n",
    "        if type(corpus)==str:\n",
    "            corpus=corpus.split(' ')\n",
    "        try:\n",
    "            self.model.train(corpus, total_examples=len(corpus), epochs=epochs)\n",
    "            self.model=self.model.wv\n",
    "            print('training completed...')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        return self.model.vocab \n",
    "        \n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>page</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Page 514 506 Fed.Appx. 514 (</td>\n",
       "      <td>2013) DAVID A. NOWICKI and BARBARA C. TREMEL,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Page 520 506 Fed.Appx. 520 (</td>\n",
       "      <td>2013) UNITED STATES OF AMERICA, Plaintiff-App...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Page 523 506 Fed.Appx. 523 (</td>\n",
       "      <td>2013) UNITED STATES OF AMERICA, Plaintiff-App...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Page 525 506 Fed.Appx. 525 (</td>\n",
       "      <td>2013) UNITED STATES OF AMERICA, Plaintiff-App...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Page 606 507 Fed.Appx. 606 (</td>\n",
       "      <td>2013) UNITED STATES OF AMERICA, Plaintiff-App...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                           page  \\\n",
       "0           0   Page 514 506 Fed.Appx. 514 (   \n",
       "1           1   Page 520 506 Fed.Appx. 520 (   \n",
       "2           2   Page 523 506 Fed.Appx. 523 (   \n",
       "3           3   Page 525 506 Fed.Appx. 525 (   \n",
       "4           4   Page 606 507 Fed.Appx. 606 (   \n",
       "\n",
       "                                             summary  \n",
       "0   2013) DAVID A. NOWICKI and BARBARA C. TREMEL,...  \n",
       "1   2013) UNITED STATES OF AMERICA, Plaintiff-App...  \n",
       "2   2013) UNITED STATES OF AMERICA, Plaintiff-App...  \n",
       "3   2013) UNITED STATES OF AMERICA, Plaintiff-App...  \n",
       "4   2013) UNITED STATES OF AMERICA, Plaintiff-App...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('sample1.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.summary=df.summary.apply(lambda x:number_special_stop(x,stop))\n",
    "# from gensim.models import Word2Vec\n",
    "# sentences=[line.split(' ') for line in df.summary[:10]]\n",
    "# text=sentences\n",
    "# word2vec_model = Word2Vec(text,window=3,workers=4, min_count=2)\n",
    "# vocab=word2vec_model.train(text,total_examples=len(text),epochs=10)\n",
    "# vocab=word2vec_model.wv.get_vector('ok')\n",
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
